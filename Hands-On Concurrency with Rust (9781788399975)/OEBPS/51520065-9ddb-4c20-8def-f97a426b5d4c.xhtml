<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<link href="Styles/Style00.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style01.css" rel="stylesheet" type="text/css" />

<style type="text/css">body{margin:1em;background-color:transparent!important;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style></head>
<body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Futures and async/await</h1>
                </header>
            
            <article>
                
<p>Asynchronous I/O is a hot topic in the Rust ecosystem right now. The C10K challenge we discussed briefly in <a href="d4802512-564b-4037-9407-b6035bd38f31.xhtml" target="_blank">Chapter 8</a>, <em>High-Level Parallelism – Threadpools, Parallel Iterators, and Processes</em>, was resolved when operating systems introduced scalable I/O notification syscalls such as kqueue in the BSDs and epoll in Linux. Prior to epoll, kqueue, and friends, I/O notification suffered linear growth in processing time as the number of file descriptors increased—a real problem. The aggressively naive approach to networking we've taken in this book also suffers from linear blowup. Every TCP socket we opened for listening would need to be polled in a loop, underserving very high-volume connections and overserving all others.</p>
<p>Rust's abstraction for scalable network I/O is mio (<a href="https://crates.io/crates/mio">https://crates.io/crates/mio</a>). If you go poking around inside of mio, you'll find that it is a clean adapter for the I/O notification subsystems of a few common platforms: Windows, Unixen, and that's about it. The user of mio doesn't get much in the way of additional support. Where do you store your mio sockets? That's up to the programmer. Can you include multithreading? Up to you. Now, that's great for very specific use cases—cernan, for example, uses mio to drive its ingress I/O because we wanted very fine control over all of these details—but unless you have very specific needs, this is probably going to be a very tedious situation for you. The community has been investing very heavily in tokio (<a href="https://crates.io/crates/tokio">https://crates.io/crates/tokio</a>), a framework for doing scalable I/O with essential things such as back-pressure, transaction cancellation, and higher-level abstractions to simplify request/response protocols. Tokio's been a fast-moving project, but will almost surely be one of the <em>key</em> projects in the coming years. Tokio is, fundamentally, a reactor-based (<a href="https://en.wikipedia.org/wiki/Reactor_pattern">https://en.wikipedia.org/wiki/Reactor_pattern</a>) architecture; an I/O event becomes available and handlers you've registered are then called.</p>
<p>Reactor-based systems are simple enough to program if you have only one event to respond to. But, more often than you'd imagine, there exist dependency relationships between I/O events in a real system—an ingressing UDP packet results in a TCP round-trip to a network memory cache, which results in an egress UDP packet to the original ingress system, and another as well. Coordinating that is hard. Tokio—and the Rust ecosystem somewhat generally—has gone all-in on a promise library called futures (<a href="https://crates.io/crates/futures">https://crates.io/crates/futures</a>). Promises are a fairly tidy solution to asynchronous I/O: the underlying reactor calls a well-established interface, you implement a closure inside (or a trait to then slot inside) that interface, and everyone's code stays loosely coupled. Rust's futures are pollable, meaning you can hit the poll function on a future and it might resolve into a value or a notification that the value isn't ready yet.</p>
<p>This is similar to promise systems in other languages. But, as anyone who remembers the early introduction of NodeJS into the JavaScript ecosystem can attest to, promises without language support get <span>weird</span><span> </span><span>and</span><span> </span><span>deeply nested fast. Rust's borrow checker made the situation harder, requiring boxing or full-on arcs where, in straight-line code, this would not have been necessary.</span></p>
<p>Languages with promises as a first-class concern generally evolve async/await syntaxes. An async function is a function in Rust that will return a future and little else. A Rust future is simple trait, extracted from the futures project. The actual implementation will live outside the language's standard library and allow for alternative implementations. The interior of an async function is not executed immediately, but only when the future is polled. The interior of the function executes through the polling until an await point is hit, resulting in a notification that the value is not yet ready and that whatever is polling the future should move on.</p>
<p>As in other languages, Rust's proposed async / await syntax really is a sugar over the top of an explicit callback chaining structure. But, because the async/await syntax and futures trait is moving into the compiler, rules can be added to the borrow-checking system to remove the current boxing concerns. I expect you'll see more futures in stable Rust once they become more convenient to interact with. Indeed, as we saw in <a href="d4802512-564b-4037-9407-b6035bd38f31.xhtml" target="_blank">Chapter 8</a>, <em>High-Level Parallelism – Pools, Iterators, and Processes</em>, rayon is investing time in a futures-based interface.</p>


            </article>

            
        </section>
    </div></body>
</html>
<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<link href="Styles/Style02.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style03.css" rel="stylesheet" type="text/css" />

<style type="text/css">body{margin:1em;background-color:transparent!important;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style></head>
<body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Read many, write exclusive locks – RwLock</h1>
                </header>
            
            <article>
                
<p>Consider a situation where you have a resource that must be manipulated only a single thread at a time, but is safe to be queried by many—that is, you have many readers and only one writer. While we could protect this resource with a <kbd>Mutex</kbd>, the trouble is that the mutex makes no distinction between its lockers; every thread will be forced to wait, no matter what their intentions. <kbd>RwLock&lt;T&gt;</kbd> is an alternative to the mutex concept, allowing for two kinds of locks—read and write. Analogously to Rust's references, there can only be one write lock taken at a time but multiple reader locks, exclusive of a write lock. Let's look at an example:</p>
<pre style="padding-left: 30px">use std::thread;
use std::sync::{Arc, RwLock};

fn main() {
    let resource: Arc&lt;RwLock&lt;u16&gt;&gt; = Arc::new(RwLock::new(0));

    let total_readers = 5;

    let mut reader_jhs = Vec::with_capacity(total_readers);
    for _ in 0..total_readers {
        let resource = Arc::clone(&amp;resource);
        reader_jhs.push(thread::spawn(move || {
            let mut total_lock_success = 0;
            let mut total_lock_failure = 0;
            let mut total_zeros = 0;
            while total_zeros &lt; 100 {
                match resource.try_read() {
                    Ok(guard) =&gt; {
                        total_lock_success += 1;
                        if *guard == 0 {
                            total_zeros += 1;
                        }
                    }
                    Err(_) =&gt; {
                        total_lock_failure += 1;
                    }
                }
            }
            (total_lock_failure, total_lock_success)
        }));
    }

    {
        let mut loops = 0;
        while loops &lt; 100 {
            let mut guard = resource.write().unwrap();
            *guard = guard.wrapping_add(1);
            if *guard == 0 {
                loops += 1;
            }
        }
    }

    for jh in reader_jhs {
        println!("{:?}", jh.join().unwrap());
    }
}</pre>
<p>The idea here is that we'll have one writer thread spinning and incrementing, in a wrapping fashion, a shared resource—a <kbd>u16.</kbd> Once the <kbd>u16</kbd> has been wrapped 100 times, the writer thread will exit. Meanwhile, a <kbd>total_readers</kbd> number of read threads will attempt to take a read lock on the shared resource—a <kbd>u16—until</kbd> it hits zero <kbd>100</kbd> times. We're gambling here, essentially, on thread ordering. Quite often, the program will exit with this result:</p>
<pre style="padding-left: 30px"><strong>(0, 100)
(0, 100)
(0, 100)
(0, 100)
(0, 100)</strong></pre>
<p>This means that each reader thread never failed to get its read lock—there were no write locks present. That is, the reader threads were scheduled before the writer. Our main function only joins on reader handlers and so the writer is left writing as we exit. Sometimes, we'll hit just the right scheduling order, and get the following result:</p>
<pre style="padding-left: 30px"><strong>(0, 100)
(126143752, 2630308)
(0, 100)
(0, 100)
(126463166, 2736405)</strong></pre>
<p>In this particular instance, the second and final reader threads were scheduled just after the writer and managed to catch a time when the guard was not zero. Recall that the first element of the pair is the total number of times the reader thread was not able to get a read lock and was forced to retry. The second is the number of times that the lock was acquired. In total, the writer thread did <kbd>(2^18 * 100) ~= 2^24</kbd> writes, whereas the second reader thread did <kbd>log_2 2630308 ~= 2^21</kbd> reads. That's a lot of lost writes, which, maybe, is okay. Of more concern, that's a lot of useless loops, approximately <kbd>2^26</kbd>. Ocean levels are rising and we're here burning up electricity like nobody had to die for it.</p>
<p>How do we avoid all this wasted effort? Well, like most things, it depends on what we're trying to do. If we need every reader to get every write, then an MPSC is a reasonable choice. It would look like this:</p>
<pre style="padding-left: 30px">use std::thread;
use std::sync::mpsc;

fn main() {
    let total_readers = 5;
    let mut sends = Vec::with_capacity(total_readers);

    let mut reader_jhs = Vec::with_capacity(total_readers);
    for _ in 0..total_readers {
        let (snd, rcv) = mpsc::sync_channel(64);
        sends.push(snd);
        reader_jhs.push(thread::spawn(move || {
            let mut total_zeros = 0;
            let mut seen_values = 0;
            for v in rcv {
                seen_values += 1;
                if v == 0 {
                    total_zeros += 1;
                }
                if total_zeros &gt;= 100 {
                    break;
                }
            }
            seen_values
        }));
    }

    {
        let mut loops = 0;
        let mut cur: u16 = 0;
        while loops &lt; 100 {
            cur = cur.wrapping_add(1);
            for snd in &amp;sends {
                snd.send(cur).expect("failed to send");
            }
            if cur == 0 {
                loops += 1;
            }
        }
    }

    for jh in reader_jhs {
        println!("{:?}", jh.join().unwrap());
    }
}</pre>
<p>It will run—for a while—and print out the following:</p>
<pre><strong>6553600
6553600
6553600
6553600
6553600</strong></pre>
<p>But what if every reader does not need to see every write, meaning that it's acceptable for a reader to miss writes so long as it does not miss all of the writes? We have options. Let's look at one.</p>


            </article>

            
        </section>
    </div></body>
</html>